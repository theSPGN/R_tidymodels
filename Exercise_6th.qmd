---
title: "Tidymodels Exercise sixth"
author: "Mateusz Zajda"
format: 
  html:
    toc: true
    toc-title: "contents"
    number-depth: 5
    number-sections: true
    code-fold: show
    code-summary: "Hide/Show code"
    code-tools: true
    code-block-border-left: "black"
    code-line-numbers: true
    code-copy: true
    html-math-method: katex
    smooth-scroll: true
    self-contained: true
    anchor-sections: true
    link-external-icon: true
    link-external-newwindow: true
    theme: 
      light: journal
      dark: darkly
    fontsize:  1.0em
    linestretch: 1.5
date: today
abstract-title: "About"
abstract: "Exercise **sixth**"
execute: 
  eval: true
  warning: false
  echo: true
  output: true
  error: false
editor_options: 
  chunk_output_type: console
---

```{=html}
<style type="text/css"> body {text-align: justify} </style>
```
## sixth exercise
o3 prediction using 3 different models

### Required packages


```{r}

# tidy data models library:
library(tidymodels)
# extension of ggplot2 for correlation graph:
library(GGally)
# data for model creation:
library(openair)
# data manipulation library:
library(dplyr)
# decision tree model library:
library(ranger)
# random forest model library:
library(rpart)
# linear regresion model library:
library(glmnet)
# model examination library:
library(yardstick)
# variable importance plots library:
library(vip)

# override with tidymodels functions:
tidymodels_prefer()
```

### Data preparation

```{r}
# selecting the data from 2002
air <-
    mydata |>
    selectByDate(year = 2002)

# removing rows with na value
air <-
    air |> na.omit()
```

### Air variables correlations

```{r}
#| label: ggpairs plot
#| layout-ncol: 1
#| fig-cap: "Drawing 1.1. ggpairs plot"
#| fig-cap-location: bottom
# correlation graph to examinate importance of variables and relations between them
air |> GGally::ggpairs()
```

### Changing wind direction into categorical data
```{r}
air <- 
    air |> mutate(
        wd = cut(
            wd,
            breaks = 16,
            labels = seq(1, 16)
        )
    )
```
### Splitting the data

```{r}
# setting seed for pseudorandom numbers
set.seed(222)

# splitting the data into the training nad testing partitions
data_split <- initial_split(
    data = air,
    prop = 3 / 4,
    strata = o3
)
train_data <- training(data_split)
test_data <- testing(data_split)

# splitting the training into the validation set and actual training set
val_set <- validation_split(
    data = train_data,
    prop = 3 / 4,
    strata = o3
)
```

### Defining models:

```{r}
# linear model:
lin_mod <-
    linear_reg(
        penalty = tune(),
        mixture = tune()
    ) |>
    set_engine(
        engine = "glmnet",
        num.threads = parallel::detectCores() - 1
    ) |>
    set_mode("regression")

# decision tree model::
dt_mod <-
    decision_tree(
        cost_complexity = tune(),
        tree_depth = tune(),
        min_n = tune()
    ) |>
    set_engine(
        engine = "rpart"
    ) |>
    set_mode("regression")

# random forest model:
rf_mod <-
    rand_forest(
        mtry = tune(),
        trees = tune(),
        min_n = tune()
    ) |>
    set_engine(
        engine = "ranger",
        num.threads = parallel::detectCores() - 1,
        importance = "impurity"
    ) |>
    set_mode("regression")
```

### Recipes

```{r}
# linear regression
lin_recipe <-
    recipe(o3 ~ ., data = train_data) |>
    update_role(date, pm10, pm25, new_role = "ID") |>
    step_date(date, features = c("month")) |> # change for new var
    step_time(date, features = c("hour")) |> # change for new var
    step_rm(date) |> # remove date column
    step_dummy(all_nominal_predictors()) |> # change categorical data into the binary 0/1 data
    step_zv(all_predictors()) |> # remove colummns with single values
    step_impute_knn(all_predictors()) |> # imputation of missing values
    step_corr(all_predictors(), threshold = 0.7) # decorelation values

lin_recipe |>
    prep() |>
    bake(train_data) |>
    glimpse()


# decision tree
dt_recipe <-
    recipe(o3 ~ ., data = train_data) |>
    update_role(date, pm10, pm25, new_role = "ID") |>
    step_date(date, features = c("month")) |> # change for new var
    step_time(date, features = c("hour")) |> # change for new var
    step_rm(date) # remove date column

dt_recipe |>
    prep() |>
    bake(train_data) |>
    glimpse()


# random forest
rf_recipe <-
    recipe(o3 ~ ., data = train_data) |>
    update_role(date, pm10, pm25, new_role = "ID") |>
    step_date(date, features = c("month")) |> # change for new var
    step_time(date, features = c("hour")) |> # change for new var
    step_rm(date) |> # remove date column
    step_zv(all_predictors()) |> # remove colummns with single values
    step_impute_knn(all_predictors()) # imputation of missing values

rf_recipe |>
    prep() |>
    bake(train_data) |>
    glimpse()
```

### Workflows

```{r}
lin_workflow <-
    workflow() |>
    add_model(lin_mod) |>
    add_recipe(lin_recipe)

dt_workflow <-
    workflow() |>
    add_model(dt_mod) |>
    add_recipe(dt_recipe)

rf_workflow <-
    workflow() |>
    add_model(rf_mod) |>
    add_recipe(rf_recipe) 
```

### Grids

```{r}
lin_grid <-
    grid_regular(
        penalty(),
        mixture(),
        levels = 5
    )

dt_grid <-
    grid_regular(
        cost_complexity(),
        tree_depth(),
        min_n(),
        levels = 5
    )

rf_grid <-
    grid_regular(
        mtry(range=c(1, 8)),
        trees(),
        min_n(),
        levels = 5
    )
```

### Tune models

```{r}
#| eval: false
lin_res <-
    lin_workflow |>
    tune_grid(
        resamples = val_set,
        grid = lin_grid,
        control = control_grid(save_pred = TRUE),
        metrics = metric_set(mae)
    )
```

```{r}
#| eval: false
dt_res <-
    dt_workflow |>
    tune_grid(
        resamples = val_set,
        grid = dt_grid,
        control = control_grid(save_pred = TRUE),
        metrics = metric_set(mae)
    )
```
```{r}
#| eval: false
rf_res <-
    rf_workflow |>
    tune_grid(
        resamples = val_set,
        grid = rf_grid,
        control = control_grid(save_pred = TRUE),
        metrics = metric_set(mae)
    )
```


```{r}
#| eval: false
#| echo: false
save(lin_res, dt_res, rf_res, file="Exercise6.RData")

```

```{r}
#| echo: false
load("Exercise6.RData")
```


```{r}
lin_top_models <-
    lin_res |>
    show_best(metric="mae", n = Inf) |>
    arrange(penalty) |>
    mutate(mean = mean |> round(x = _, digits = 3))

lin_top_models |> gt::gt()
```

```{r}
dt_top_models <-
    dt_res |>
    show_best(metric="mae", n = Inf) |>
    arrange(tree_depth) |>
    mutate(mean = mean |> round(x = _, digits = 3))

dt_top_models |> gt::gt()
```

```{r}
rf_top_models <-
    rf_res |>
    show_best(metric="mae", n = Inf) |>
    arrange(trees) |>
    mutate(mean = mean |> round(x = _, digits = 3))

rf_top_models |> gt::gt()
```

### Best models selection

```{r}
lin_best <-
    lin_res |>
    select_best()

dt_best <-
    dt_res |>
    select_best()

rf_best <-
    rf_res |>
    select_best()

lin_res |>
    show_best(metric = "mae", n = Inf) |>
    filter(.config == lin_best$.config) |>
    select(
        penalty, 
        mixture, 
        .metric, 
        mean)

dt_res |>
    show_best(metric = "mae", n = Inf) |>
    filter(.config == dt_best$.config) |>
    select(
        cost_complexity, 
        tree_depth, 
        min_n, 
        .metric, 
        mean)

rf_res |>
    show_best(metric = "mae", n = Inf) |>
    filter(.config == rf_best$.config) |>
    select(
        mtry, 
        trees, 
        min_n, 
        .metric, 
        mean)

```

### Final models

```{r}
lin_best_mod <-
    lin_workflow |>
    finalize_workflow(lin_best)

dt_best_mod <-
    dt_workflow |>
    finalize_workflow(dt_best)

rf_best_mod <-
    rf_workflow |>
    finalize_workflow(rf_best)
```

```{r}
lin_fit <-
    lin_best_mod |>
    last_fit(split = data_split)

dt_fit <-
    dt_best_mod |>
    last_fit(split = data_split)

rf_fit <-
    rf_best_mod |>
    last_fit(split = data_split)
```

```{r}
lin_fit |>
    extract_fit_parsnip() |>
    vip(num_features = 20) +
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_boxplot(color = "black", fill = "grey85") 

dt_fit |>
    extract_fit_parsnip() |>
    vip(num_features = 20) +
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_boxplot(color = "black", fill = "grey85") 

rf_fit |>
    extract_fit_parsnip() |>
    vip(num_features = 20) +
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_boxplot(color = "black", fill = "grey85")

bind_rows(
    lin_fit |> collect_metrics(),
    dt_fit |> collect_metrics(),
    rf_fit |> collect_metrics()
) |> knitr::kable(digits = 3)
```
